{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1 | Mask Recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Goal :*** *Detect human faces on videos and check whether or not they have a mask on*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement two different models to perform the task :\n",
    "- [Faster-RCNN (ResNet50)](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. INITIALIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1.1 IMPORTS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import time\n",
    "\n",
    "from tools import engine, utils\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "cv2                 4.5.4\n",
       "numpy               1.21.4\n",
       "pandas              1.3.4\n",
       "session_info        1.0.0\n",
       "tools               NA\n",
       "torch               1.10.0+cu102\n",
       "torchvision         0.11.1+cu102\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "PIL                         8.4.0\n",
       "astunparse                  1.6.3\n",
       "backcall                    0.2.0\n",
       "cffi                        1.15.0\n",
       "colorama                    0.4.4\n",
       "cycler                      0.10.0\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.4.3\n",
       "decorator                   5.1.0\n",
       "defusedxml                  0.7.1\n",
       "entrypoints                 0.3\n",
       "google                      NA\n",
       "ipykernel                   6.4.1\n",
       "ipython_genutils            0.2.0\n",
       "jedi                        0.18.0\n",
       "kiwisolver                  1.3.2\n",
       "matplotlib                  3.4.3\n",
       "mpl_toolkits                NA\n",
       "nt                          NA\n",
       "ntsecuritycon               NA\n",
       "parso                       0.8.2\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "prompt_toolkit              3.0.20\n",
       "pycocotools                 NA\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.4.1\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pyexpat                     NA\n",
       "pygments                    2.10.0\n",
       "pyparsing                   3.0.6\n",
       "pythoncom                   NA\n",
       "pytz                        2021.3\n",
       "pywin32_bootstrap           NA\n",
       "pywin32_system32            NA\n",
       "pywintypes                  NA\n",
       "six                         1.16.0\n",
       "storemagic                  NA\n",
       "tornado                     6.1\n",
       "tqdm                        4.62.3\n",
       "traitlets                   5.1.0\n",
       "typing_extensions           NA\n",
       "wcwidth                     0.2.5\n",
       "win32api                    NA\n",
       "win32com                    NA\n",
       "win32security               NA\n",
       "zmq                         22.3.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             7.29.0\n",
       "jupyter_client      7.0.2\n",
       "jupyter_core        4.7.1\n",
       "notebook            6.4.3\n",
       "-----\n",
       "Python 3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]\n",
       "Windows-10-10.0.22000-SP0\n",
       "-----\n",
       "Session information updated at 2021-12-06 22:39\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to fill the `requirement.txt` file we use the following line of code:\n",
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1.3. DATA LOADING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "!python ./DataPreprocessing.py\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_height</th>\n",
       "      <th>image_width</th>\n",
       "      <th>box_id</th>\n",
       "      <th>box_label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>109</td>\n",
       "      <td>105</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>185</td>\n",
       "      <td>226</td>\n",
       "      <td>100</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>325</td>\n",
       "      <td>360</td>\n",
       "      <td>90</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>402</td>\n",
       "      <td>432</td>\n",
       "      <td>105</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>111</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>37</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8910</th>\n",
       "      <td>5182</td>\n",
       "      <td>266</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>184</td>\n",
       "      <td>88</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8911</th>\n",
       "      <td>5183</td>\n",
       "      <td>266</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>184</td>\n",
       "      <td>88</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>5184</td>\n",
       "      <td>266</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>184</td>\n",
       "      <td>88</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8913</th>\n",
       "      <td>5185</td>\n",
       "      <td>266</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>184</td>\n",
       "      <td>88</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8914</th>\n",
       "      <td>5186</td>\n",
       "      <td>266</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>184</td>\n",
       "      <td>88</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8915 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id  image_height  image_width  box_id  box_label  xmin  xmax  \\\n",
       "0            0           366          512       0          1    79   109   \n",
       "1            0           366          512       1          3   185   226   \n",
       "2            0           366          512       2          1   325   360   \n",
       "3            1           366          512       0          1   402   432   \n",
       "4            2           111           90       0          1    30    60   \n",
       "...        ...           ...          ...     ...        ...   ...   ...   \n",
       "8910      5182           266          276       0          2    92   184   \n",
       "8911      5183           266          276       0          2    92   184   \n",
       "8912      5184           266          276       0          2    92   184   \n",
       "8913      5185           266          276       0          2    92   184   \n",
       "8914      5186           266          276       0          2    92   184   \n",
       "\n",
       "      ymin  ymax  \n",
       "0      105   142  \n",
       "1      100   144  \n",
       "2       90   141  \n",
       "3      105   142  \n",
       "4       37    74  \n",
       "...    ...   ...  \n",
       "8910    88   177  \n",
       "8911    88   177  \n",
       "8912    88   177  \n",
       "8913    88   177  \n",
       "8914    88   177  \n",
       "\n",
       "[8915 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir_path = \"data/FaceMaskDetection_Processed/\" # path to the directory with the relevant data\n",
    "images_dir_path = data_dir_path + \"images/\"         # path to the directory with the images\n",
    "images_files = os.listdir(images_dir_path)           # list of files in the image directory\n",
    "\n",
    "annotations = pd.read_csv(data_dir_path + \"annotations.csv\", index_col=None) # dataframe with information about the images and their bounding boxes\n",
    "display(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMaskDataset1(Dataset):\n",
    "\n",
    "    def __init__(self, annotations, images_dir_path, images_files):\n",
    "        self.annotations = annotations\n",
    "        self.images_dir_path = images_dir_path\n",
    "        self.images_files = images_files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_id = int(self.annotations.iloc[idx][\"image_id\"])\n",
    "        img = cv2.imread(self.images_dir_path+str(img_id)+\".png\")\n",
    "        xmin = self.annotations.iloc[idx][\"xmin\"]\n",
    "        xmax = self.annotations.iloc[idx][\"xmax\"]\n",
    "        ymin = self.annotations.iloc[idx][\"ymin\"]\n",
    "        ymax = self.annotations.iloc[idx][\"ymax\"]\n",
    "        img = transforms.Resize((256,256))(torch.Tensor(img[ymin:ymax,xmin:xmax]).permute(2,0,1))\n",
    "        label = torch.zeros(3) \n",
    "        label[int(self.annotations.iloc[idx][\"box_label\"])-1] = 1\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMaskDataset2(Dataset):\n",
    "\n",
    "    def __init__(self, annotations, images_dir_path, images_files):\n",
    "        self.annotations = annotations\n",
    "        self.images_dir_path = images_dir_path\n",
    "        self.images_files = images_files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = cv2.imread(self.images_dir_path+self.images_files[idx])\n",
    "        img = transforms.Resize((256,256))(torch.Tensor(img).permute(2,0,1))\n",
    "\n",
    "        img_id = int(self.images_files[idx][:-4])\n",
    "        img_annotations = self.annotations[self.annotations[\"image_id\"] == img_id]\n",
    "\n",
    "        img_height = int(list(img_annotations[\"image_height\"])[0])\n",
    "        img_width = int(list(img_annotations[\"image_width\"])[0])\n",
    "        xmins = [256*xmin/img_width for xmin in list(img_annotations[\"xmin\"])]\n",
    "        ymins = [256*ymin/img_height for ymin in list(img_annotations[\"ymin\"])]\n",
    "        xmaxs = [256*xmax/img_width for xmax in list(img_annotations[\"xmax\"])]\n",
    "        ymaxs = [256*ymax/img_height for ymax in list(img_annotations[\"ymax\"])]\n",
    "\n",
    "        target =  {\n",
    "            \"boxes\": torch.as_tensor([[xmins[i], ymins[i], xmaxs[i], ymaxs[i]] for i in range(len(img_annotations))], dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(list(img_annotations[\"box_label\"]), dtype=torch.int64),\n",
    "            \"image_id\": torch.as_tensor([img_id]),\n",
    "            \"area\": torch.as_tensor([(xmaxs[i]-xmins[i])*(ymaxs[i]-ymins[i]) for i in range(len(img_annotations))], dtype=torch.float32),\n",
    "            \"iscrowd\": torch.zeros((len(img_annotations),), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% of the whole dataset is dedicated to training and the 10% left is used as a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMD1 = FaceMaskDataset1(annotations, images_dir_path, images_files)\n",
    "FMD2 = FaceMaskDataset2(annotations, images_dir_path, images_files)\n",
    "\n",
    "train_ratio = 0.9\n",
    "trainset1, testset1 = torch.utils.data.random_split(FMD1, [int(train_ratio*len(FMD1)), len(FMD1)-int(train_ratio*len(FMD1))])\n",
    "trainset2, testset2 = torch.utils.data.random_split(FMD2, [int(train_ratio*len(FMD2)), len(FMD2)-int(train_ratio*len(FMD2))])\n",
    "\n",
    "batch_size1 = 128\n",
    "batch_size2 = 2\n",
    "\n",
    "trainloader1 = DataLoader(trainset1, batch_size=batch_size1, shuffle=True)\n",
    "testloader1 = DataLoader(testset1, batch_size=batch_size1, shuffle=True)\n",
    "\n",
    "trainloader2 = DataLoader(trainset2, batch_size=batch_size2, shuffle=True, collate_fn=utils.collate_fn)\n",
    "testloader2 = DataLoader(testset2, batch_size=batch_size2, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. THE MODELS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.1. Defining the models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "FaceDetection_model = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRSN18 = models.resnet18(pretrained=True)\n",
    "for param in modelRSN18.parameters():\n",
    "    param.requires_grad = False\n",
    "modelRSN18.fc = nn.Sequential(\n",
    "    nn.Linear(512,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,3),\n",
    "    nn.Softmax(dim=1))\n",
    "modelRSN18.to(device)\n",
    "try:\n",
    "    modelRSN18.load_state_dict(torch.load(\"./models/MaskRecognitionFasterRSN18.pt\"))\n",
    "    print(\"model loaded\")\n",
    "except:\n",
    "    print(\"new model\")\n",
    "    pass\n",
    "modelRSN18.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained on COCO\n",
    "modelRCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# get number of input features for the classifier\n",
    "in_features = modelRCNN.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "modelRCNN.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=4)\n",
    "modelRCNN.to(device)\n",
    "try:\n",
    "    modelRCNN.load_state_dict(torch.load(\"./models/MaskRecognitionFasterRCNN.pt\"))\n",
    "    print(\"model loaded\")\n",
    "except:\n",
    "    print(\"new model\")\n",
    "    pass\n",
    "modelRCNN.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.2. Training the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerRSN18 = torch.optim.Adam(modelRSN18.parameters(), lr=1e-4)\n",
    "criterionRSN18 = nn.CrossEntropyLoss().cuda()\n",
    "num_epochsRSN18 = 10\n",
    "epoch_print_frequence = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochsRSN18):\n",
    "\n",
    "    running_loss_train, running_loss_test, running_acc_train, running_acc_test = 0,0,0,0\n",
    "\n",
    "    for train in [True, False]:\n",
    "\n",
    "        if train:\n",
    "            dataloader = trainloader1\n",
    "            modelRSN18.train()\n",
    "        else:\n",
    "            dataloader = testloader1\n",
    "            modelRSN18.eval()\n",
    "\n",
    "        for inputs,labels in dataloader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if train:\n",
    "                optimizerRSN18.zero_grad()\n",
    "\n",
    "            outputs = modelRSN18(inputs)\n",
    "            loss = criterionRSN18(outputs, labels)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizerRSN18.step()\n",
    "                running_loss_train += loss.item()\n",
    "                running_acc_train += np.sum(torch.argmax(labels, dim=1).cpu().detach().numpy() == torch.argmax(outputs, dim=1).cpu().detach().numpy())\n",
    "                \n",
    "            else:\n",
    "                running_loss_test += loss.item()\n",
    "                running_acc_test += np.sum(torch.argmax(labels, dim=1).cpu().detach().numpy() == torch.argmax(outputs, dim=1).cpu().detach().numpy()) \n",
    "\n",
    "    running_loss_train /= len(trainloader1)\n",
    "    running_loss_test /= len(testloader1)\n",
    "    running_acc_train /= len(trainset1)\n",
    "    running_acc_test /= len(testset1)\n",
    "\n",
    "    train_losses.append(running_loss_train)\n",
    "    test_losses.append(running_loss_test)\n",
    "    train_accuracies.append(running_acc_train)\n",
    "    test_accuracies.append(running_acc_test)\n",
    "\n",
    "    if (epoch+1) % epoch_print_frequence == 0:\n",
    "        print(\"epochs {} ({} s) | train loss : {} | test loss : {} | train acc : {} | test acc : {}\".format(\n",
    "            epoch+1,\n",
    "            int(time.time()-s),\n",
    "            int(1000000*running_loss_train)/1000000,\n",
    "            int(1000000*running_loss_test)/1000000,\n",
    "            int(1000000*running_acc_train)/1000000,\n",
    "            int(1000000*running_acc_test)/1000000)\n",
    "        )\n",
    "\n",
    "resultsRSN18 = train_losses, test_losses, train_accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsRCNN = [p for p in modelRCNN.parameters() if p.requires_grad]\n",
    "optimizerRCNN = torch.optim.SGD(paramsRCNN, lr=0.0005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_schedulerRCNN = torch.optim.lr_scheduler.StepLR(optimizerRCNN, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochsRCNN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochsRCNN):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    engine.train_one_epoch(modelRCNN, optimizerRCNN, trainloader2, device, epoch, print_freq=len(trainloader2)//3)\n",
    "    # update the learning rate\n",
    "    lr_schedulerRCNN.step()\n",
    "    # evaluate on the test dataset\n",
    "    engine.evaluate(modelRCNN, testloader2, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.3. Saving the models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelRSN18.state_dict(), \"./models/MaskRecognitionFasterRSN18.pt\")\n",
    "torch.save(modelRCNN.state_dict(), \"./models/MaskRecognitionFasterRCNN.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.4. Testing the models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRSN18 = models.resnet18(pretrained=True)\n",
    "for param in modelRSN18.parameters():\n",
    "    param.requires_grad = False\n",
    "modelRSN18.fc = nn.Sequential(\n",
    "    nn.Linear(512,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,3),\n",
    "    nn.Softmax(dim=1))\n",
    "modelRSN18.to(device)\n",
    "modelRSN18.load_state_dict(torch.load(\"./models/MaskRecognitionFasterRSN18.pt\"))\n",
    "modelRSN18.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = modelRCNN.roi_heads.box_predictor.cls_score.in_features\n",
    "modelRCNN.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=4)\n",
    "modelRCNN.to(device)\n",
    "modelRCNN.load_state_dict(torch.load(\"./models/MaskRecognitionFasterRCNN.pt\"))\n",
    "modelRCNN.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes_RSN18(img):\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    boxes = FaceDetection_model.detectMultiScale(gray_img, 1.1, 4)\n",
    "    results = []\n",
    "    for (x,y,w,h) in boxes:\n",
    "        model_input = transforms.Resize((256,256))(torch.Tensor(img[y:y+h,x:x+w]).permute(2,0,1))\n",
    "        model_input = model_input.reshape((1,3,256,256)).to(device)\n",
    "        label = torch.argmax(modelRSN18(model_input)).item()\n",
    "        results.append((x,y,x+w,y+h,label))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes_RCNN(img):\n",
    "    h,w,c = img.shape\n",
    "    results = []\n",
    "    model_input = transforms.Resize((256,256))(torch.Tensor(img).permute(2,0,1))\n",
    "    model_input = model_input.reshape((1,3,256,256)).to(device)\n",
    "    target = modelRCNN(model_input)[0]\n",
    "    for i in range(len(target[\"boxes\"])):\n",
    "        box = target[\"boxes\"][i]\n",
    "        label = int(target[\"labels\"][i])\n",
    "        xmin = int(w*box[0]/256)\n",
    "        ymin = int(h*box[1]/256)\n",
    "        xmax = int(w*box[2]/256)\n",
    "        ymax = int(h*box[3]/256)\n",
    "        results.append((xmin,ymin,xmax,ymax,label))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_results(get_boxes, nb_images):\n",
    "\n",
    "    img_ids = rd.sample(list(annotations[\"image_id\"]), nb_images)\n",
    "\n",
    "    for img_id in img_ids:\n",
    "\n",
    "        img = cv2.imread(images_dir_path+\"{}.png\".format(img_id))\n",
    "        cv2.imshow(\"before | {}.png\".format(img_id), img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        h,w,c = img.shape\n",
    "        overlay = img.copy()\n",
    "        output = img.copy()\n",
    "\n",
    "        boxes = get_boxes(img)\n",
    "\n",
    "        for (xmin,ymin,xmax,ymax,label) in boxes:\n",
    "            \n",
    "            if label == 1:\n",
    "                cv2.rectangle(overlay, (xmin,ymin), (xmax,ymax), (0,0,255), 2)\n",
    "            \n",
    "            elif label == 2:\n",
    "                cv2.rectangle(overlay, (xmin,ymin), (xmax,ymax), (0,255,255), 2)\n",
    "            \n",
    "            else:\n",
    "                cv2.rectangle(overlay, (xmin,ymin), (xmax,ymax), (0,255,0), 2)\n",
    "\n",
    "        output = cv2.addWeighted(overlay, 0.5, output, 0.5, 0, output)        \n",
    "        cv2.imshow(\"after | maksssksksss{}.png\".format(img_id), output)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_results(get_boxes_RSN18, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_results(get_boxes_RCNN, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
